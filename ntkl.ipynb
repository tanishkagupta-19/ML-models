{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9c698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sentence.', 'This is another one.']\n",
      "['This', 'is', 'a', 'sentence', '.', 'This', 'is', 'another', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "print(sent_tokenize('This is a sentence. This is another one.'))\n",
    "print(word_tokenize('This is a sentence. This is another one.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b73a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', 'This', 'another', 'one']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words=set(stopwords.words('english'))\n",
    "words=word_tokenize('This is a sentence. This is another one.')\n",
    "filtered_note = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "print(filtered_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ee6378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'runner', 'ran', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "words=['running','runner','ran','easily','fairly']\n",
    "stemmed_words=[ps.stem(w) for w in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2048db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ac5c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('Tanishka', 'NNP'), ('and', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Hello I am Tanishka and it is good to see you\"\n",
    "print(pos_tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b19f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, word_tokenize\n",
    "text=\"Hello I am Tanishka and it is good to see you\"\n",
    "chunked=ne_chunk(pos_tag(word_tokenize(text)))\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "357e6b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AI', 3)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "text=\"AI is the future. AI will change everything. Learn AI.\"\n",
    "fdist=FreqDist(word_tokenize(text))\n",
    "print(fdist.most_common(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f53983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "sentence = pos_tag(word_tokenize(\"The big red dog barked.\"))\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"  # Chunk rule: NP = optional Determiner + any Adjectives + Noun\n",
    "parser = RegexpParser(grammar)\n",
    "tree = parser.parse(sentence)\n",
    "tree.draw()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
